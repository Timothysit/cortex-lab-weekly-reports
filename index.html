<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>From 2020-01-08 to 2020-01-17</title>
    <link rel="stylesheet" href="./css/reveal.css" />
    <link rel="stylesheet" href="./css/theme/white.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />
    <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template">




# Progress update 2020 Jan 8 - 2020 Jan 17

</script></section><section  data-markdown><script type="text/template">

## B2 Rig Upgrade

Ordered parts for B2 upgrade, will do it next week. 

 - Stimulus computer: Main upgrade includes (1) 4 &rightarrow; 16 GB ram (2) SSD (3) GPU
 - Master / eye camera computer: (1) DDR2 ram &rightarrow; DDR4 ram (2) SSD 
 - Should also add microphones (and maybe another camera for tracking movement) soon...

</script></section><section  data-markdown><script type="text/template">

# 2P Analysis

</script></section><section  data-markdown><script type="text/template">


## 2P analysis: finding neurons that respond to audio 

TS003-plane-7-cell-24


![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/audio-marignal-response-bar-chart/audio_marginal_reponse_cell_24.png) <!-- .element height="70%" width="100%"; -->

</script></section><section  data-markdown><script type="text/template">

### TS003-plane-7-cell-24

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/test-audio-video-pair-cell-24.png) <!-- .element height="70%" width="100%"; -->


</script></section><section  data-markdown><script type="text/template">

## 2P analysis: Decoding video and audio

Simple SVM classifier with default parameters.
Using about 400 cells from one plane.

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/av-classification/simple-svm-cv-split-classfication-accuracy.png) <!-- .element height="50%" width="50%"; -->

</script></section><section  data-markdown><script type="text/template">

### Video decoding

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/av-classification/video-decoding-performance-diff-classifiers.png)

</script></section><section  data-markdown><script type="text/template">

### Audio decoding

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/av-classification/audio-decoding-performance-diff-classifiers.png)


</script></section><section  data-markdown><script type="text/template">

### Using all cells 

Caveat: some planes weren't manually curated yet, so may be noisy.

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/av-classification/knn-classifier-w-diff-PCs-subject-TS003-exp-2.png) <!-- .element height="50%" width="80%"; -->



</script></section><section ><section data-markdown><script type="text/template">

### Some neurons seem to respond strongly to a specific audio-visual pair in both repeats 

subject-3-exp-1-plane-7-cell-74

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/test-audio-video-pair-cell-74.png) <!-- .element height="50%" width="80%"; -->

</script></section><section data-markdown><script type="text/template">

subject-3-exp-1-plane-7-cell-176 

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/filmworld/2p/TS003/2019-11-28/exp-2/test-audio-video-pair-cell-176.png) <!-- .element height="50%" width="80%"; -->


</script></section></section><section  data-markdown><script type="text/template">


# Ephys Analysis 

</script></section><section  data-markdown><script type="text/template">

## Classifying cells 

<iframe width="100%" height="600pt" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTk1w56bgGRBI4aoc4njVzvys9-WC60yz7livo-JODzwurYubD0fk5pxAZ6lnBqduOSZeu2rrOMwB4P/pubhtml?widget=true&amp; headers=false";></iframe>

</script></section><section  data-markdown><script type="text/template">

## Main cell types 

 1. Go sustained firing 
 2. No-go enhanced 
	- a lot also seems to respond to audio in passive condition 
 3. Trial-onset / auditory neurons 
 4. Movement / choice-related neurons that also respond to audio in passive condition 
 5. Trial/audio onset suppressed cells 
    - suppressed: subject-3-brain-loc-1-cell-48
	- enhanced w/ sharp peak: subject-3-brain-loc-1-cell-53
 
 </script></section><section  data-markdown><script type="text/template">
 
### Go sustianed firing neurons 

subject-2-brain-loc-2-cell-20

![go-sustain-1](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-2/brain-area-2/exp-15/sig_idx_0_combined_exp_cell_idx_20.png)


</script></section><section  data-markdown><script type="text/template">

### No-go enhanced neurons

subject-2-brain-loc-3-cell-5

![no-go-enhanced-cells](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-2/brain-area-3/exp-15/sig_idx_18_combined_exp_cell_idx_5.png)

</script></section><section  data-markdown><script type="text/template">

### Trial-onset neurons 


subject-2-brain-loc-3-cell-23


![trial-onset-neuron](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-2/brain-area-3/exp-15/sig_idx_2_combined_exp_cell_idx_23.png)




</script></section><section  data-markdown><script type="text/template">


## Some rare but intesting cell types

 </script></section><section ><section data-markdown><script type="text/template">

### 1. Audio-visual interaction 

eg. subject-2-brain-loc-2-cell-80
 ![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-2/brain-area-3/exp-15/sig_idx_4_combined_exp_cell_idx_1.png) <!-- .element height="70%" width="70%"; margin-left="auto"; margin-right="auto" -->
 
 </script></section><section data-markdown><script type="text/template">
 
subject-2-brain-loc-3-cell-1

![image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-2/brain-area-3/exp-15/sig_idx_4_combined_exp_cell_idx_1.png) <!-- .element height="70%" width="70%"; margin-left="auto"; margin-right="auto" -->


 
 </script></section></section><section  data-markdown><script type="text/template">
 
### 2. Audio neurons with no movement response
  
subject-3-brain-loc-2-cell-21

![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-3/exp-21/brain-area-2/exp-21/sig_idx_58_combined_exp_cell_idx_21.png) <!-- .element height="70%" width="70%"; margin-left="auto"; margin-right="auto" -->


</script></section><section  data-markdown><script type="text/template">

## $f(a_{l}v_l) = f(a_l) + f(v_l)?$

<iframe frameborder="0" width="700pt" height="700pt" src="https://timothysit.github.io/cortex-lab-weekly-reports/figures/multispaceworld-ephys/linked_selection_subsets_stretch.html"></iframe>

</script></section><section  data-markdown><script type="text/template">

## Cell 89

![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/combined-alignment-subsubset-m-300p750bin80/subject-3/exp-21/brain-area-2/exp-21/sig_idx_3_combined_exp_cell_idx_89.png) <!-- .element height="70%" width="70%"; -->



</script></section><section  data-markdown><script type="text/template">

## First attempt at neural trajectories 

### Without PCA 

![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/multispaceworld-ephys/pop-analysis/mean_left_right_grid_trajectory_noPreprocessing.png) <!-- .element height="70%" width="70%"; -->


</script></section><section  data-markdown><script type="text/template">

### With PCA 

![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/multispaceworld-ephys/pop-analysis/mean_left_right_grid_trajectory_PCA_20.png) <!-- .element height="70%" width="70%"; -->


</script></section><section  data-markdown><script type="text/template">

### With PCA + Whitening 

(will need to smooth the firing rates)

![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/multispaceworld-ephys/pop-analysis/mean_left_right_grid_trajectory_PCA_20_whiten.png) <!-- .element height="70%" width="70%"; -->

</script></section><section  data-markdown><script type="text/template">

### Explained variance 

<div id="left"> 

#### Left choice

![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/multispaceworld-ephys/pop-analysis/left_movement_matrix_PCA_explained_variance.png) <!-- .element height="70%" width="100%"; -->

</div> 


<div id="right"> 

#### Right choice


![Image](https://timothysit.github.io/cortex-lab-weekly-reports/figures/multispaceworld-ephys/pop-analysis/right_movement_matrix_PCA_explained_variance.png) <!-- .element height="70%" width="100%"; -->

</div> 

## About demixed PCA 

 - option 1: just have the stimulus dimension 
 - option 2: stimulus dimension divided into audio and video dimension, but dPCA requires all combination of audio-visual condition to be present







<style>

#left {
	margin: 10px 0 15px 20px;
	text-align: left;
	float: left;
	z-index:-10;
	width:48%;
	font-size: 0.85em;
	line-height: 1.5; 
}

#right {
	margin: 10px 0 15px 0;
	float: right;
	text-align: left;
	z-index:-10;
	width:48%;
	font-size: 0.85em;
	line-height: 1.5; 
}

</style>


</script></section></div>
    </div>

    <script src="./js/reveal.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // Optional libraries used to extend on reveal.js
      var deps = [
        { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
        { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: './plugin/zoom-js/zoom.js', async: true },
        { src: './plugin/notes/notes.js', async: true },
        { src: './plugin/math/math.js', async: true }
      ];

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        dependencies: deps
      };

      // options from URL query string
      var queryOptions = Reveal.getQueryHash() || {};

      var options = extend(defaultOptions, {"transition":"fade"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
